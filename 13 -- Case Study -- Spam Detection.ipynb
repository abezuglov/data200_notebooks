{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection Filter\n",
    "\n",
    "Let us illustrate a few concepts from before on a case study -- building a spam detection filter. The data for this notebook is from https://medium.com/analytics-vidhya/building-a-spam-filter-from-scratch-using-machine-learning-fc58b178ea56 \n",
    "\n",
    "## Read Data\n",
    "\n",
    "Here we will read the data and place them to Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mnonspam-test\u001b[0m/\r\n",
      "\u001b[01;34mnonspam-train\u001b[0m/\r\n",
      "\u001b[01;34mspam-test\u001b[0m/\r\n",
      "\u001b[01;34mspam-train\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls data/email | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/email/spam-train/spmsga95.txt',\n",
       " './data/email/spam-train/spmsgc4.txt',\n",
       " './data/email/spam-train/spmsgc108.txt',\n",
       " './data/email/spam-train/spmsga32.txt',\n",
       " './data/email/spam-train/spmsgc50.txt']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = \"./data/email\"\n",
    "train_spam, train_legit = \"./data/email/spam-train/*\", \"./data/email/nonspam-train/*\"\n",
    "test_spam,test_legit = \"./data/email/spam-test/*\", \"./data/email/nonspam-test/*\"\n",
    "file_paths = glob.glob('./data/email/spam-train/*')\n",
    "file_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 2), (260, 2))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_emails(dir_):\n",
    "     return [' '.join(open(file_path, \"r\").readlines()).strip() for file_path in glob.glob(dir_)]\n",
    "\n",
    "def read_df(spam_dir, legit_dir):\n",
    "    # spam\n",
    "    df_spam = pd.DataFrame({'email':read_emails(spam_dir)})\n",
    "    df_spam['spam'] = 1\n",
    "    # legit\n",
    "    df_legit = pd.DataFrame({'email':read_emails(legit_dir)})\n",
    "    df_legit['spam'] = 0\n",
    "    return pd.concat([df_spam,df_legit])\n",
    "\n",
    "df_train = read_df(train_spam, train_legit)\n",
    "df_test = read_df(test_spam, test_legit)\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.to_csv('./data/email/train.csv')\n",
    "#df_test.to_csv('./data/email/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer\n",
    "\n",
    "Apply TfidfVectorizer to convert email texts into word vectors. There are multiple options to improve the model (TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "X_train = tf.fit_transform(df_train['email'])\n",
    "y_train = df_train['spam']\n",
    "X_test = tf.transform(df_test['email'])\n",
    "y_test = df_test['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 19073), (700,), (260, 19073), (260,))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "We will first apply principal component analysis to see how many components are needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.todense() # we have to convert the data to dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = PCA()\n",
    "\n",
    "p.fit(X_train.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many components are needed to explain 95% of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_comps = np.argmax(np.cumsum(p.explained_variance_) > 0.95)\n",
    "n_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PCA(n_components = n_comps)\n",
    "X_train_pca = p.fit_transform(X_train.todense())\n",
    "X_test_pca = p.transform(X_test.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/jh/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state = 1)\n",
    "lr.fit(X_train_pca, y_train)\n",
    "y_pred = lr.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       legit       0.97      0.99      0.98       130\n",
      "        spam       0.99      0.97      0.98       130\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       260\n",
      "   macro avg       0.98      0.98      0.98       260\n",
      "weighted avg       0.98      0.98      0.98       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred, target_names = ['legit','spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes = [100,100,10]\n",
    "                   ,learning_rate_init = 0.0002\n",
    "                   ,max_iter = 1000\n",
    "                   ,verbose = 1, random_state = 2)\n",
    "nn.fit(X_train_pca, y_train)\n",
    "y_pred = nn.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       legit       0.99      0.99      0.99       130\n",
      "        spam       0.99      0.99      0.99       130\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       260\n",
      "   macro avg       0.99      0.99      0.99       260\n",
      "weighted avg       0.99      0.99      0.99       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred, target_names = ['legit','spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with the model\n",
    "\n",
    "Let us put all the steps into a pipeline:\n",
    "* TfidFVectorizer to convert the text to word counts;\n",
    "* Transform the sparse matrix to dense;\n",
    "* Apply principal component analysis;\n",
    "* Run artificial neural network to predict if the email is spam;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('tfidfvectorizer',tf)\n",
    "                 ,('todense',FunctionTransformer(lambda x: x.todense(), accept_sparse=True, validate = False))\n",
    "                 ,('pca',p)\n",
    "                 ,('ann',nn)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method returns two numbers which can be thought of 'probabilities' that the email is legit (1st) vs. spam (2nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02945142,  0.97054858]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(['Hello'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type a sample email below and see the spam confidence changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1000ce94374f98a9a25688df206e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='Hi there!', description='x'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(lambda x:print(\"Spam: %.5f\"%model.predict_proba([x])[0][1])\n",
    "         , x='Hi there!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
